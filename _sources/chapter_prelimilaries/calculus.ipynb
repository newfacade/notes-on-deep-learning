{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculus\n",
    "\n",
    "```{note}\n",
    "Differential calculus can tell us how to increase or decrease a function's value by manipulating its arguments. This comes in handy for the optimization problems that we face in deep learning, where we repeatedly update our parameters in order to decrease the loss function.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives and Differentiation\n",
    "\n",
    "Put simply, a derivative is the rate of change in a function with respect to changes in its arguments. For function $f:\\mathbb{R}\\to\\mathbb{R}$\n",
    "\n",
    "$$f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h}.$$\n",
    "\n",
    "When $f'(x)$ exits, $f$ is said to be differentiable at $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Derivatives and Gradients\n",
    "\n",
    "Let $y = f(x_1, x_2, \\ldots, x_n)$ be a function with $n$ variables. The partial derivative of $y$ with respect to $x_{i}$ is\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}.$$\n",
    "\n",
    "We can treat $x_1, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_n$ as constants \n",
    "and calculate the derivative of $y$ with respect to $x_i$. \n",
    "\n",
    "We can concatenate partial derivatives of a multivariant function with respect to all its variables to obtain a vector that is called the gradient of the function. Suppose that the input of the function $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ is a $n-$dimensional vector $\\mathbf{x}=[x_{1},x_{2},\\ldots,x_{n}]^\\top$ and the output is a scalar, the gradient is\n",
    "\n",
    "$$\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\left[\\frac{\\partial y}{\\partial x_1}, \\frac{\\partial y}{\\partial x_2}, \\ldots\n",
    "\\frac{\\partial y}{\\partial x_n})\\right]^\\top.$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量求导\n",
    "\n",
    "我们使用常用的分子布局符号。<br>\n",
    "标量关于`列向量`的导数是一个`行向量`：\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3\\\\\n",
    "a & b & c\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\\mathbf{x} = \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n} \\end{bmatrix}$$\n",
    "$$\\frac{\\partial{y}}{\\partial\\mathbf{x}}=\\begin{bmatrix}\n",
    "  \\frac{\\partial y}{\\partial x_{1}}& \\frac{\\partial y}{\\partial x_{2}}& \\ldots  &\\frac{\\partial y}{\\partial x_{n}}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "`列向量`关于标量的导数是一个`列向量`：\n",
    "\n",
    "$$\\mathbf{x} = \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n} \\end{bmatrix}$$\n",
    "\n",
    "$$\\frac{\\partial{\\mathbf{y}}}{\\partial{x}}=\\begin{bmatrix}\n",
    "  \\frac{\\partial y_{1}}{\\partial x}\\\\ \\frac{\\partial y_{2}}{\\partial x}\\\\\\vdots  \\\\\\frac{\\partial y_{m}}{\\partial x}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "（列）向量关于（列）向量的导数是一个矩阵：\n",
    "\n",
    "$$\\mathbf{x} =\\begin{bmatrix}x_{1}  \\\\x_{2}  \\\\ \\vdots  \\\\x_{n}\\end{bmatrix}\\quad\\mathbf{y} =\\begin{bmatrix}y_{1}  \\\\y_{2}  \\\\ \\vdots  \\\\y_{m}\\end{bmatrix}\\quad\\frac{\\partial{\\mathbf{y}}}{\\partial\\mathbf{x}}=\\begin{bmatrix}\n",
    "  \\frac{\\partial y_{1}}{\\partial \\mathbf{x}}\\\\ \\frac{\\partial y_{2}}{\\partial \\mathbf{x}}\\\\\\vdots  \\\\\\frac{\\partial y_{m}}{\\partial \\mathbf{x}}\\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial{y_1}}{\\partial{x_1}} & \\cdots & \\frac{\\partial{y_1}}{\\partial{x_n}}  \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial{y_m}}{\\partial{x_1}} & \\cdots & \\frac{\\partial{y_m}}{\\partial{x_n}} \\\\ \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常见的求导结果\n",
    "\n",
    "标量关于向量的导数：\n",
    "\n",
    "$$\\frac{\\partial{\\text{sum}(\\mathbf{x})}}{\\partial\\mathbf{x}}=\\mathbf{1}^{T},\\quad \\frac{\\partial\\left \\| \\mathbf{x} \\right \\|^{2} }{\\partial\\mathbf{x}}=2\\mathbf{x}^{T}$$\n",
    "\n",
    "向量对向量求导：\n",
    "\n",
    "$$\\frac{\\partial{\\mathbf{x}}}{\\partial{\\mathbf{x}}}=\\mathbf{I},\\quad\\frac{\\partial{\\mathbf{A}\\mathbf{x}}}{\\partial{\\mathbf{x}}}=\\mathbf{A}$$\n",
    "\n",
    "\n",
    "\n",
    "例.（线性回归）假设$\\mathbf{X}\\in\\mathbb{R}^{m\\times{n}}, \\mathbf{w}\\in\\mathbb{R}^{n}, \\mathbf{y}\\in\\mathbb{R}^{m}$，损失函数$z=\\left \\|\\mathbf{X}\\mathbf{w}-\\mathbf{y}  \\right \\|^{2} $，求$\\frac{\\partial{z}}{\\partial{\\mathbf{w}}}$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{\\partial{z}}{\\partial{\\mathbf{w}}} &= \\frac{\\partial{\\left \\|\\mathbf{X}\\mathbf{w}-\\mathbf{y}  \\right \\|^{2}}}{\\partial{\\mathbf{X}\\mathbf{w}-\\mathbf{y}}}\\frac{\\partial{\\mathbf{X}\\mathbf{w}-\\mathbf{y}}}{\\partial{\\mathbf{X}\\mathbf{w}}}\\frac{\\partial{\\mathbf{X}\\mathbf{w}}}{\\partial{\\mathbf{w}}}\\quad(\\mbox{链式法则})\\\\\n",
    "&= 2(\\mathbf{X}\\mathbf{w}-\\mathbf{y})^{T}\\mathbf{X}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵求导\n",
    "\n",
    "类似地，我们可以把导数扩展到矩阵，导数的shape还是分子同分母反：\n",
    "\n",
    "![image](../images/diff.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
