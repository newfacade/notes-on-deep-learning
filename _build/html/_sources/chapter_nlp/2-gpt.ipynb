{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598b19f1-18af-4707-8b52-f9312898b3e2",
   "metadata": {},
   "source": [
    "# GPT,GPT2,GPT3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c61590c-8e11-4e79-b154-e82dd3ba1d55",
   "metadata": {},
   "source": [
    "## GPT\n",
    "\n",
    "paper: https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\n",
    "\n",
    "GPT 使用了预训练 + 微调的方式。\n",
    "\n",
    "### 模型结构\n",
    "\n",
    "在预训练阶段，GPT 选择使用 transformer decoder。\n",
    "\n",
    "![](../images/nlp/gpt.png)\n",
    "\n",
    "### 预训练\n",
    "\n",
    "GPT 使用标准的语言模型目标函数来最大化下面的似然函数：\n",
    "\n",
    "$$L_{1}(\\mathcal{U}) = \\sum_{i}\\log P(u_{i}|u_{i-k},\\dots,u_{i-1};\\Theta)$$\n",
    "\n",
    "具体来说就是要预测每个词 $u_{i}$ 的概率，这个概率基于它前面的 $k$ 个词以及 $\\Theta$，这里 $k$ 表示上文的窗口大小。\n",
    "\n",
    "模型输入为 $U$，那么模型可以表示为：\n",
    "\n",
    "$$h_{0} = UW_{e} + W_{p}$$\n",
    "$$h_{l} = transformerBlock(h_{l-1})$$\n",
    "$$P(u) = softmax(h_{n}W_{e}^{T})$$\n",
    "\n",
    "其中 $W_{e}$ 是词嵌入矩阵，$W_{p}$ 是位置嵌入矩阵。\n",
    "\n",
    "### 微调\n",
    "\n",
    "微调时可让模型认识特殊标识符，微调目标和 $L_{1}(\\mathcal{U})$ 一起训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91b9b54-dcee-4c01-b0b4-ea7ab3c31c9a",
   "metadata": {},
   "source": [
    "## GPT2\n",
    "\n",
    "paper: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "\n",
    "GPT-2 的核心思想就是，当模型的容量非常大且数据量足够丰富时，仅仅靠语言模型的学习便可以完成其他有监督学习的任务，不需要在下游任务微调。\n",
    "\n",
    "GPT-2 主推 zero-shot。\n",
    "\n",
    "### 模型结构\n",
    "\n",
    "GPT2 相比 GPT 做了如下调整：\n",
    "\n",
    "1. 后置层归一化（ post-norm ）改为前置层归一化（ pre-norm ）\n",
    "2. 在模型最后一个自注意力层之后，额外增加一个层归一化\n",
    "3. 调整参数的初始化方式，按残差层个数进行缩放，缩放比例为 $\\frac{1}{\\sqrt{n}}$\n",
    "4. 输入序列的最大长度从 512 扩充到 1024\n",
    "\n",
    "![](../images/nlp/gpt-norm.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a88dd2-dc76-4516-bd57-9c85510403ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
